{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db5dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataset import Dataset\n",
    "from feature_extractor import FeatureExtractor\n",
    "from evaluation import get_auroc, get_average_precision, get_tnr_frac_tpr\n",
    "from monitors import OutsideTheBoxMonitor, GaussianMixtureMonitor, MaxSoftmaxProbabilityMonitor, MaxLogitMonitor, \\\n",
    "    EnergyMonitor, ReActMonitor, MahalanobisMonitor\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6b3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# id_dataset = \"cifar10\"\n",
    "model = \"densenet\"\n",
    "layer = 98\n",
    "\n",
    "# novelties = [\"cifar100\", \"svhn\", \"lsun\"]\n",
    "\n",
    "all_id_datasets = [\"cifar10\", \"svhn\", \"cifar100\"]\n",
    "\n",
    "all_ood_datasets = [[\"cifar100\", \"svhn\", \"lsun\"],\n",
    "                    [\"cifar10\", \"tiny_imagenet\", \"lsun\"],\n",
    "                    [\"cifar10\", \"svhn\", \"lsun\"]]\n",
    "\n",
    "perturbations = [\"brightness\", \"blur\", \"pixelization\"]\n",
    "attacks = [\"fgsm\", \"deepfool\", \"pgd\"]\n",
    "\n",
    "cov_constraints = [\"full\", \"diag\", \"tied\", \"spherical\"]\n",
    "n_clusters = [1, 2, 3, 5]\n",
    "is_novel = [1, 1, 1, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d8e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_parameters_gmm(features_train, pred_train, lab_train,\n",
    "                          features_test, pred_test, lab_test,\n",
    "                          features_ood, pred_ood, lab_ood,\n",
    "                          is_novel_ood, eval_idx,\n",
    "                          cov_constraints_list, n_clusters_list, \n",
    "                          selection_metric = \"aupr\"):\n",
    "    \n",
    "    print(\"Tuning GMM hyperparameters for dataset \", eval_idx)\n",
    "    \n",
    "    correct_indices_train = (lab_train == pred_train)\n",
    "    \n",
    "    features_train = features_train[correct_indices_train]\n",
    "    lab_train = lab_train[correct_indices_train]\n",
    "    pred_train = pred_train[correct_indices_train]\n",
    "    \n",
    "    N = features_test.shape[0] // 2\n",
    "    features_test_selec = features_test[N:]\n",
    "    pred_test_selec = pred_test[N:]\n",
    "    lab_test_selec = lab_test[N:]\n",
    "\n",
    "    features_test_eval = features_test[:N]\n",
    "    pred_test_eval = pred_test[:N]\n",
    "    lab_test_eval = lab_test[:N]\n",
    "    \n",
    "    n_ood_datasets = len(lab_ood)\n",
    "    select_indices = [k for k in range(n_ood_datasets) if k is not eval_idx]\n",
    "    \n",
    "    features_ood_selec = [features_ood[i][:N] for i in select_indices]\n",
    "    pred_ood_selec = [pred_ood[i][:N] for i in select_indices]\n",
    "    lab_ood_selec = [lab_ood[i][:N] for i in select_indices]\n",
    "    is_novel_ood_selec = [is_novel_ood[i] for i in select_indices]\n",
    "        \n",
    "    features_ood_eval = features_ood[eval_idx][:N]\n",
    "    pred_ood_eval = pred_ood[eval_idx][:N]\n",
    "    lab_ood_eval = lab_ood[eval_idx][:N]\n",
    "    is_novel_ood_eval = is_novel_ood[eval_idx]\n",
    "    \n",
    "    if selection_metric == \"aupr\":\n",
    "        metric = get_average_precision\n",
    "    elif selection_metric == \"auroc\":\n",
    "        metric = get_auroc\n",
    "    elif selection_metric == \"tnr95tpr\":\n",
    "        metric = get_tnr_frac_tpr\n",
    "    else:\n",
    "        \"Error, unsupported selection metric. Valid choices are 'aupr', 'auroc' and 'tnr95tpr'\"\n",
    "        \n",
    "    constraints, n_components = [], []\n",
    "    for cat in list(set(lab_train)):\n",
    "        print(\"... Selecting hyperparameters for class \", cat)\n",
    "        \n",
    "        f_train = features_train[pred_train == cat]\n",
    "        \n",
    "        res_config = []\n",
    "        for cc in  cov_constraints:\n",
    "            for n in n_clusters:\n",
    "                gmm = GaussianMixture(n_components=n, covariance_type=cc)\n",
    "                gmm.fit(f_train)\n",
    "                \n",
    "                res = []\n",
    "                for i in range(len(features_ood_selec)):\n",
    "                    f = np.concatenate([features_test_selec[pred_test_selec == cat], \n",
    "                                        features_ood_selec[i][pred_ood_selec[i] == cat]])\n",
    "                    p = np.concatenate([pred_test_selec[pred_test_selec == cat], \n",
    "                                        pred_ood_selec[i][pred_ood_selec[i] == cat]])\n",
    "                    if is_novel_ood_selec[i]:\n",
    "                        l = np.concatenate([lab_test_selec[pred_test_selec == cat], \n",
    "                                            -np.ones(lab_ood_selec[i][pred_ood_selec[i] == cat].shape)])\n",
    "                    else:\n",
    "                        l = np.concatenate([lab_test_selec[pred_test_selec == cat], \n",
    "                                            lab_ood_selec[i][pred_ood_selec[i] == cat]])\n",
    "                    try:\n",
    "                        scores = -gmm.score_samples(f)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    res.append(metric(scores, l, p))\n",
    "#                     print(cc, n, res[-1])\n",
    "                \n",
    "                res = np.array(res)\n",
    "                res_config.append(res)\n",
    "                \n",
    "        res_config = np.array(res_config)\n",
    "        \n",
    "        ranks = np.zeros(res_config.shape)\n",
    "        for i in range(res_config.shape[1]):\n",
    "            temp = res_config[:, i].argsort()\n",
    "            ranks[:, i] = np.empty_like(temp)\n",
    "            ranks[temp, i] = np.arange(len(res_config))\n",
    "        avg_ranks = np.mean(ranks, axis = 1)\n",
    "\n",
    "        constraints.append(cov_constraints[np.argmax(avg_ranks) // len(n_clusters)])\n",
    "        n_components.append(n_clusters[np.argmax(avg_ranks) % len(n_clusters)])\n",
    "                \n",
    "    f_eval = np.concatenate([features_test_eval, features_ood_eval])\n",
    "    p_eval = np.concatenate([pred_test_eval, pred_ood_eval])\n",
    "    if is_novel_ood_eval:\n",
    "        l_eval = np.concatenate([lab_test_eval, -np.ones(lab_ood_eval.shape)])\n",
    "    else:\n",
    "        l_eval = np.concatenate([lab_test_eval, lab_ood_eval])\n",
    "        \n",
    "    monitor = GaussianMixtureMonitor(id_dataset, model, layer, n_components=n_components, constraint=constraints)\n",
    "    monitor.fit(features_train, pred_train, lab_train, save=False)\n",
    "    \n",
    "    scores = -monitor.predict(f_eval, p_eval)\n",
    "    \n",
    "    ap = get_average_precision(scores, l_eval, p_eval)\n",
    "    auroc = get_auroc(scores, l_eval, p_eval)\n",
    "    tnr = get_tnr_frac_tpr(scores, l_eval, p_eval)\n",
    "    \n",
    "    print(\"AP       = \", ap)\n",
    "    print(\"AUROC    = \", auroc)\n",
    "    print(\"TNR95TPR = \", tnr)\n",
    "    \n",
    "    return constraints, n_components, ap, auroc, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeeb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID dataset:  cifar10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "... OOD dataset:  cifar100\n",
      "Tuning GMM hyperparameters for dataset  0\n",
      "... Selecting hyperparameters for class  0\n",
      "... Selecting hyperparameters for class  1\n",
      "... Selecting hyperparameters for class  2\n",
      "... Selecting hyperparameters for class  3\n",
      "... Selecting hyperparameters for class  4\n",
      "... Selecting hyperparameters for class  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joris/Softwares/miniconda3/envs/neural-network-monitoring-benchmark/lib/python3.10/site-packages/sklearn/mixture/_base.py:286: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Selecting hyperparameters for class  6\n",
      "... Selecting hyperparameters for class  7\n",
      "... Selecting hyperparameters for class  8\n",
      "... Selecting hyperparameters for class  9\n",
      "AP       =  0.8971359573284498\n",
      "AUROC    =  0.8917531399829207\n",
      "TNR95TPR =  0.6020189003436427\n",
      "['tied', 'full', 'tied', 'tied', 'tied', 'tied', 'tied', 'tied', 'tied', 'diag'] [2, 5, 5, 3, 5, 2, 3, 5, 5, 3] 0.8971359573284498 0.8917531399829207 0.6020189003436427\n",
      "\n",
      "... OOD dataset:  svhn\n",
      "Tuning GMM hyperparameters for dataset  1\n",
      "... Selecting hyperparameters for class  0\n",
      "... Selecting hyperparameters for class  1\n",
      "... Selecting hyperparameters for class  2\n",
      "... Selecting hyperparameters for class  3\n",
      "... Selecting hyperparameters for class  4\n",
      "... Selecting hyperparameters for class  5\n",
      "... Selecting hyperparameters for class  6\n",
      "... Selecting hyperparameters for class  7\n",
      "... Selecting hyperparameters for class  8\n",
      "... Selecting hyperparameters for class  9\n",
      "AP       =  0.9301235201518434\n",
      "AUROC    =  0.9507453360032513\n",
      "TNR95TPR =  0.8481529209621993\n",
      "['tied', 'full', 'tied', 'tied', 'tied', 'tied', 'tied', 'tied', 'tied', 'full'] [3, 5, 3, 3, 5, 3, 5, 3, 2, 5] 0.9301235201518434 0.9507453360032513 0.8481529209621993\n",
      "\n",
      "... OOD dataset:  lsun\n",
      "Tuning GMM hyperparameters for dataset  2\n",
      "... Selecting hyperparameters for class  0\n",
      "... Selecting hyperparameters for class  1\n",
      "... Selecting hyperparameters for class  2\n",
      "... Selecting hyperparameters for class  3\n",
      "... Selecting hyperparameters for class  4\n",
      "... Selecting hyperparameters for class  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joris/Softwares/miniconda3/envs/neural-network-monitoring-benchmark/lib/python3.10/site-packages/sklearn/mixture/_base.py:286: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Selecting hyperparameters for class  6\n",
      "... Selecting hyperparameters for class  7\n",
      "... Selecting hyperparameters for class  8\n",
      "... Selecting hyperparameters for class  9\n",
      "AP       =  0.9458366957912194\n",
      "AUROC    =  0.9397508140934626\n",
      "TNR95TPR =  0.7167096219931272\n",
      "['full', 'spherical', 'tied', 'tied', 'tied', 'tied', 'tied', 'tied', 'tied', 'diag'] [2, 1, 3, 5, 5, 2, 5, 3, 5, 3] 0.9458366957912194 0.9397508140934626 0.7167096219931272\n",
      "\n",
      "... OOD dataset:  brightness\n",
      "Tuning GMM hyperparameters for dataset  3\n",
      "... Selecting hyperparameters for class  0\n",
      "... Selecting hyperparameters for class  1\n"
     ]
    }
   ],
   "source": [
    "for i_id, id_dataset in enumerate(all_id_datasets):\n",
    "    print(\"ID dataset: \", id_dataset)\n",
    "    \n",
    "    train_dataset = Dataset(id_dataset, \"train\", model, batch_size=batch_size)\n",
    "    test_dataset = Dataset(id_dataset, \"test\", model, batch_size=batch_size)\n",
    "    \n",
    "    feature_extractor = FeatureExtractor(model, id_dataset, [layer], device_name)\n",
    "    \n",
    "    novelties = all_ood_datasets[i_id]\n",
    "    \n",
    "    ood_names = [novelties[0], novelties[1], novelties[2], \n",
    "                 perturbations[0], perturbations[1], perturbations[2],\n",
    "                 attacks[0], attacks[1], attacks[2]]\n",
    "    \n",
    "    ood_datasets = []\n",
    "    ood_datasets.append(Dataset(novelties[0], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(novelties[1], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(novelties[2], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[0], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[1], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[2], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[0], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[1], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[2], batch_size=batch_size))\n",
    "\n",
    "    features_train, logits_train, softmax_train, pred_train, lab_train = feature_extractor.get_features(train_dataset)\n",
    "    features_train = features_train[0]\n",
    "    features_test, logits_test, softmax_test, pred_test, lab_test = feature_extractor.get_features(test_dataset)\n",
    "    features_test = features_test[0]\n",
    "\n",
    "    features_ood, logits_ood, softmax_ood, pred_ood, lab_ood = [], [], [], [], []\n",
    "    for k in range(len(ood_datasets)):\n",
    "        features, logits, softmax, pred, lab = feature_extractor.get_features(ood_datasets[k])\n",
    "        features_ood.append(features[0])\n",
    "        logits_ood.append(logits)\n",
    "        softmax_ood.append(softmax)\n",
    "        pred_ood.append(pred)\n",
    "        lab_ood.append(lab)\n",
    "\n",
    "    for eval_idx in range(9):\n",
    "        print(\"... OOD dataset: \", ood_names[eval_idx])\n",
    "        constraints, n_components, ap, auroc, tnr = select_parameters_gmm(features_train, pred_train, lab_train,\n",
    "                                                                          features_test, pred_test, lab_test,\n",
    "                                                                          features_ood, pred_ood, lab_ood,\n",
    "                                                                          is_novel, eval_idx,\n",
    "                                                                          cov_constraints, n_clusters, \n",
    "                                                                          selection_metric = \"tnr95tpr\")\n",
    "        print(constraints, n_components, ap, auroc, tnr)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e70f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2999647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mahalanobis(features_train, pred_train, lab_train,\n",
    "                     features_test, pred_test, lab_test,\n",
    "                     features_ood, pred_ood, lab_ood,\n",
    "                     is_novel_ood, eval_idx):\n",
    "    \n",
    "    correct_indices_train = (lab_train == pred_train)\n",
    "    \n",
    "    features_train = features_train[correct_indices_train]\n",
    "    lab_train = lab_train[correct_indices_train]\n",
    "    pred_train = pred_train[correct_indices_train]\n",
    "    \n",
    "    N = features_test.shape[0] // 2\n",
    "\n",
    "    features_test_eval = features_test[:N]\n",
    "    pred_test_eval = pred_test[:N]\n",
    "    lab_test_eval = lab_test[:N]\n",
    "    \n",
    "    features_ood_eval = features_ood[eval_idx][:N]\n",
    "    pred_ood_eval = pred_ood[eval_idx][:N]\n",
    "    lab_ood_eval = lab_ood[eval_idx][:N]\n",
    "    is_novel_ood_eval = is_novel_ood[eval_idx]\n",
    "          \n",
    "    f_eval = np.concatenate([features_test_eval, features_ood_eval])\n",
    "    p_eval = np.concatenate([pred_test_eval, pred_ood_eval])\n",
    "    if is_novel_ood_eval:\n",
    "        l_eval = np.concatenate([lab_test_eval, -np.ones(lab_ood_eval.shape)])\n",
    "    else:\n",
    "        l_eval = np.concatenate([lab_test_eval, lab_ood_eval])\n",
    "        \n",
    "    monitor = MahalanobisMonitor(id_dataset, model, layer, is_tied=True)\n",
    "    monitor.fit(features_train, pred_train, lab_train, save=False)\n",
    "    \n",
    "    scores = -monitor.predict(f_eval, p_eval)\n",
    "    \n",
    "    ap = get_average_precision(scores, l_eval, p_eval)\n",
    "    auroc = get_auroc(scores, l_eval, p_eval)\n",
    "    tnr = get_tnr_frac_tpr(scores, l_eval, p_eval)\n",
    "    \n",
    "    print(ap, \",\", auroc, \",\", tnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "def8b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID dataset:  cifar10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0.6588314100290246 , 0.6202829072681704 , 0.10821052631578942\n",
      "0.958238852315053 , 0.9457901152882205 , 0.6336842105263158\n",
      "0.9455471620125726 , 0.9309913182957394 , 0.5741052631578947\n",
      "0.44688559132855415 , 0.6913949650878644 , 0.18148921363952675\n",
      "0.9274594621999217 , 0.9447058367028759 , 0.8672599886385155\n",
      "0.5995855138595496 , 0.8163230958714781 , 0.4407281337871837\n",
      "0.4613397496227938 , 0.7156830296054418 , 0.21087588210875885\n",
      "0.4485499300626769 , 0.6913617895518684 , 0.16822691071678075\n",
      "0.8281884302239098 , 0.8408507649878907 , 0.3953254080193431\n",
      "ID dataset:  svhn\n",
      "Using downloaded and verified file: ./Data/train_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "0.9366948473230106 , 0.9567470039004614 , 0.8489466964570699\n",
      "0.9409421768232394 , 0.9597296468541486 , 0.8657037982764124\n",
      "0.9180243376570766 , 0.9492296439923411 , 0.8459942547079476\n",
      "0.8208582595632367 , 0.9348060020832326 , 0.7147637598641494\n",
      "0.29591767365406596 , 0.8492575881336897 , 0.5115580405574565\n",
      "0.20230029354950996 , 0.8448196005742912 , 0.385966315957915\n",
      "0.7588889332479317 , 0.9285990494889074 , 0.6986567022987418\n",
      "0.6929814860482437 , 0.865280216010488 , 0.4885844748858448\n",
      "0.8837712256709153 , 0.9370824433135029 , 0.7183928786238422\n",
      "ID dataset:  cifar100\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0.5858962077007653 , 0.5055493547451309 , 0.08850012863390788\n",
      "0.8532400450410347 , 0.8053440076400082 , 0.24517622845382048\n",
      "0.9299237653511176 , 0.8822097222151497 , 0.29740159506045794\n",
      "0.6585049938445455 , 0.67845461116011 , 0.15100060642813828\n",
      "0.8970093905171574 , 0.8692958689365975 , 0.2801461632155907\n",
      "0.6353457256814005 , 0.7135498331493917 , 0.17292659675881794\n",
      "0.629418022956728 , 0.6642096893724141 , 0.15166835187057637\n",
      "0.5922342396778408 , 0.6101315228998314 , 0.09813562794509323\n",
      "0.7994842144810923 , 0.7649589843439393 , 0.22970595627041968\n"
     ]
    }
   ],
   "source": [
    "for i_id, id_dataset in enumerate(all_id_datasets):\n",
    "    print(\"ID dataset: \", id_dataset)\n",
    "    \n",
    "    train_dataset = Dataset(id_dataset, \"train\", model, batch_size=batch_size)\n",
    "    test_dataset = Dataset(id_dataset, \"test\", model, batch_size=batch_size)\n",
    "    \n",
    "    feature_extractor = FeatureExtractor(model, id_dataset, [layer], device_name)\n",
    "    \n",
    "    novelties = all_ood_datasets[i_id]\n",
    "    \n",
    "    ood_names = [novelties[0], novelties[1], novelties[2], \n",
    "                 perturbations[0], perturbations[1], perturbations[2],\n",
    "                 attacks[0], attacks[1], attacks[2]]\n",
    "    \n",
    "    ood_datasets = []\n",
    "    ood_datasets.append(Dataset(novelties[0], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(novelties[1], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(novelties[2], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[0], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[1], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[2], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[0], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[1], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[2], batch_size=batch_size))\n",
    "\n",
    "    features_train, logits_train, softmax_train, pred_train, lab_train = feature_extractor.get_features(train_dataset)\n",
    "    features_train = features_train[0]\n",
    "    features_test, logits_test, softmax_test, pred_test, lab_test = feature_extractor.get_features(test_dataset)\n",
    "    features_test = features_test[0]\n",
    "\n",
    "    features_ood, logits_ood, softmax_ood, pred_ood, lab_ood = [], [], [], [], []\n",
    "    for k in range(len(ood_datasets)):\n",
    "        features, logits, softmax, pred, lab = feature_extractor.get_features(ood_datasets[k])\n",
    "        features_ood.append(features[0])\n",
    "        logits_ood.append(logits)\n",
    "        softmax_ood.append(softmax)\n",
    "        pred_ood.append(pred)\n",
    "        lab_ood.append(lab)\n",
    "\n",
    "    for eval_idx in range(9):\n",
    "#         print(\"... OOD dataset: \", ood_names[eval_idx])\n",
    "        eval_mahalanobis(features_train, pred_train, lab_train,\n",
    "                         features_test, pred_test, lab_test,\n",
    "                         features_ood, pred_ood, lab_ood,\n",
    "                         is_novel, eval_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f7f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec24ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_MSP(softmax_test, pred_test, lab_test,\n",
    "             softmax_ood, pred_ood, lab_ood,\n",
    "             is_novel_ood, eval_idx):\n",
    "    \n",
    "    N = softmax_test.shape[0] // 2\n",
    "\n",
    "    softmax_test_eval = softmax_test[:N]\n",
    "    pred_test_eval = pred_test[:N]\n",
    "    lab_test_eval = lab_test[:N]\n",
    "    \n",
    "    softmax_ood_eval = softmax_ood[eval_idx][:N]\n",
    "    pred_ood_eval = pred_ood[eval_idx][:N]\n",
    "    lab_ood_eval = lab_ood[eval_idx][:N]\n",
    "    is_novel_ood_eval = is_novel_ood[eval_idx]\n",
    "          \n",
    "    s_eval = np.concatenate([softmax_test_eval, softmax_ood_eval])\n",
    "    p_eval = np.concatenate([pred_test_eval, pred_ood_eval])\n",
    "    if is_novel_ood_eval:\n",
    "        l_eval = np.concatenate([lab_test_eval, -np.ones(lab_ood_eval.shape)])\n",
    "    else:\n",
    "        l_eval = np.concatenate([lab_test_eval, lab_ood_eval])\n",
    "        \n",
    "    monitor = MaxSoftmaxProbabilityMonitor()\n",
    "    monitor.fit()\n",
    "    \n",
    "    scores = -monitor.predict(s_eval)\n",
    "    \n",
    "    ap = get_average_precision(scores, l_eval, p_eval)\n",
    "    auroc = get_auroc(scores, l_eval, p_eval)\n",
    "    tnr = get_tnr_frac_tpr(scores, l_eval, p_eval)\n",
    "    \n",
    "    print(ap, \",\", auroc, \",\", tnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aabfd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID dataset:  cifar10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0.9064580782796519 , 0.9143132230576441 , 0.6812631578947368\n",
      "0.910390839581521 , 0.9220553784461153 , 0.7294736842105263\n",
      "0.9621038076120779 , 0.9671329323308269 , 0.8846315789473684\n",
      "0.5867067666681072 , 0.8182140610186756 , 0.4629088378566458\n",
      "0.7597432344061327 , 0.8190328661658127 , 0.4090134444234047\n",
      "0.6642986472353485 , 0.8435565113982139 , 0.5588278821962409\n",
      "0.6664938089597721 , 0.8748798618555421 , 0.6305520963055209\n",
      "0.34841115069457085 , 0.654007713338213 , 0.5365376554422243\n",
      "0.5515102274587463 , 0.575356646529964 , 0.0\n",
      "ID dataset:  svhn\n",
      "Using downloaded and verified file: ./Data/train_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "0.9251078461334415 , 0.9255823634037877 , 0.5355888924353654\n",
      "0.9515485655504532 , 0.9517811244115484 , 0.7427385892116183\n",
      "0.9460084622150768 , 0.9453995482362558 , 0.6813756782636451\n",
      "0.62708825160905 , 0.8751612494143297 , 0.652282489261812\n",
      "0.4759077910157882 , 0.8660808624576974 , 0.28719853125260786\n",
      "0.3657670632190633 , 0.8442556488421123 , 0.18122174660959312\n",
      "0.7525234706208896 , 0.9249358413717816 , 0.6049096584996689\n",
      "0.29714145704722145 , 0.6561192487303382 , 0.473430678910131\n",
      "0.5247920661080474 , 0.5506531077227542 , 0.0\n",
      "ID dataset:  cifar100\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0.878894770684673 , 0.8409122827011782 , 0.47311551324929246\n",
      "0.9140588105440615 , 0.8915130912198952 , 0.6395677900694623\n",
      "0.8556716071306464 , 0.8078783881188647 , 0.4059686133264728\n",
      "0.8652318869735227 , 0.8817970187130225 , 0.6054174247018396\n",
      "0.8614026900856352 , 0.8483820285776567 , 0.5863580998781973\n",
      "0.8172770181390899 , 0.8554568919976866 , 0.5530981887511917\n",
      "0.8447427072544926 , 0.8670147887894435 , 0.5801820020222447\n",
      "0.729761068813855 , 0.7697292254224483 , 0.5140340094242983\n",
      "0.6779898829847106 , 0.5920746558528771 , 0.12716763005780352\n"
     ]
    }
   ],
   "source": [
    "for i_id, id_dataset in enumerate(all_id_datasets):\n",
    "    print(\"ID dataset: \", id_dataset)\n",
    "    \n",
    "    train_dataset = Dataset(id_dataset, \"train\", model, batch_size=batch_size)\n",
    "    test_dataset = Dataset(id_dataset, \"test\", model, batch_size=batch_size)\n",
    "    \n",
    "    feature_extractor = FeatureExtractor(model, id_dataset, [layer], device_name)\n",
    "    \n",
    "    novelties = all_ood_datasets[i_id]\n",
    "    \n",
    "    ood_names = [novelties[0], novelties[1], novelties[2], \n",
    "                 perturbations[0], perturbations[1], perturbations[2],\n",
    "                 attacks[0], attacks[1], attacks[2]]\n",
    "    \n",
    "    ood_datasets = []\n",
    "    ood_datasets.append(Dataset(novelties[0], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(novelties[1], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(novelties[2], \"test\", model, batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[0], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[1], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, additional_transform=perturbations[2], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[0], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[1], batch_size=batch_size))\n",
    "    ood_datasets.append(Dataset(id_dataset, \"test\", model, adversarial_attack=attacks[2], batch_size=batch_size))\n",
    "\n",
    "    features_train, logits_train, softmax_train, pred_train, lab_train = feature_extractor.get_features(train_dataset)\n",
    "    features_train = features_train[0]\n",
    "    features_test, logits_test, softmax_test, pred_test, lab_test = feature_extractor.get_features(test_dataset)\n",
    "    features_test = features_test[0]\n",
    "\n",
    "    features_ood, logits_ood, softmax_ood, pred_ood, lab_ood = [], [], [], [], []\n",
    "    for k in range(len(ood_datasets)):\n",
    "        features, logits, softmax, pred, lab = feature_extractor.get_features(ood_datasets[k])\n",
    "        features_ood.append(features[0])\n",
    "        logits_ood.append(logits)\n",
    "        softmax_ood.append(softmax)\n",
    "        pred_ood.append(pred)\n",
    "        lab_ood.append(lab)\n",
    "\n",
    "    for eval_idx in range(9):\n",
    "#         print(\"... OOD dataset: \", ood_names[eval_idx])\n",
    "        eval_MSP(softmax_test, pred_test, lab_test,\n",
    "                 softmax_ood, pred_ood, lab_ood,\n",
    "                 is_novel, eval_idx)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4211c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
